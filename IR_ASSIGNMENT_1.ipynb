{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***ANSWER 1***"
      ],
      "metadata": {
        "id": "8YJq-0PXllwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "SfILB_EbNB_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -la /content/drive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU_-sR8cRDUC",
        "outputId": "41c66435-ccff-408f-abba-ac33c4131c4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 12\n",
            "drwxr-xr-x 3 root root 4096 Feb  9 14:08 .\n",
            "drwxr-xr-x 1 root root 4096 Feb  9 14:08 ..\n",
            "drwxr-xr-x 3 root root 4096 Feb  9 14:08 MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive.mount('/content/drive', force_remount=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1z8v4wO5NMyU",
        "outputId": "7bba0729-11cd-47f1-c569-5027e630c248"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from nltk import download\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Configuration for directories\n",
        "src_dir = '/content/drive/My Drive/IR/text_files'\n",
        "dest_dir = '/content/drive/MyDrive/IR/processed_files'\n",
        "\n",
        "# Ensure necessary NLTK resources\n",
        "def setup_nltk():\n",
        "    download('punkt')\n",
        "    download('stopwords')\n",
        "\n",
        "# Clean and transform text\n",
        "def transform_content(raw_text):\n",
        "    # Convert to lowercase\n",
        "    lower_text = raw_text.lower()\n",
        "    # Tokenize\n",
        "    words = word_tokenize(lower_text)\n",
        "    # Filter out stopwords and punctuation\n",
        "    filtered_words = [w for w in words if w not in set(stopwords.words('english')) | set(string.punctuation)]\n",
        "\n",
        "    # Combine back to string\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# Function to print file content\n",
        "def print_file_content(file_path, label):\n",
        "    \"\"\"Print the entire content of a file with a label.\"\"\"\n",
        "    with open(file_path, 'r') as file:\n",
        "        content = file.read()\n",
        "    print(f\"--- {label} ---\\n{content}\")\n",
        "\n",
        "# Process individual files\n",
        "def handle_file(input_path, output_path):\n",
        "    with open(input_path, 'r') as input_file:\n",
        "        content = input_file.read()\n",
        "        cleaned_content = transform_content(content)\n",
        "\n",
        "    with open(output_path, 'w') as output_file:\n",
        "        output_file.write(cleaned_content)\n",
        "\n",
        "    # Print file content before and after processing\n",
        "    print_file_content(input_path, \"Before Processing\")\n",
        "    print_file_content(output_path, \"After Processing\")\n",
        "    print(\"\\n\" + \"=\"*60 + \"\\n\")  # Add separator\n",
        "\n",
        "# Main function to iterate through all files\n",
        "def process_files(folder_source, folder_destination):\n",
        "    setup_nltk()\n",
        "    files = [filename for filename in os.listdir(folder_source) if filename.endswith('.txt')]\n",
        "    selected_files = random.sample(files, min(len(files), 5))  # Select 5 random files\n",
        "\n",
        "    for filename in selected_files:\n",
        "        src_file_path = os.path.join(folder_source, filename)\n",
        "        dest_file_path = os.path.join(folder_destination, filename[:-4] + '_cleaned.txt')\n",
        "        handle_file(src_file_path, dest_file_path)\n",
        "\n",
        "# Execute processing\n",
        "if __name__ == \"__main__\":\n",
        "    if not os.path.exists(dest_dir):\n",
        "        os.makedirs(dest_dir)\n",
        "    process_files(src_dir, dest_dir)\n"
      ],
      "metadata": {
        "id": "J1_VF7VulOnr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acfb4a0b-c5a9-4c5e-a259-78af0daa2ef7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Before Processing ---\n",
            "I'm a long time musician, and a long time user of Ernie Ball Strings (on electric guitars). Unfortunately, I can't endorse these strings, even though they sound great and the light gauge saves my fingers. The problem is that they break, in my humble opinion, excessively easily. I put a set on and broke a g string within 2 days of moderate use. It snapped near the saddle, so I didn't think much of it. I replaced the set, and within a few more days I noticed that the wrapping on the g string was broken and becoming unraveled near my third fret. I bought 7 sets, so I still have 5 more sets to go through, but honestly I'll probably go back to elixirs when these are gone, since they seem to last longer.\n",
            "\n",
            "*update*\n",
            "I purchased these strings on 12/19. It' now 1/18, and I've broken 3 strings from 3 different packs.\n",
            "At this point, I really hate these strings.\n",
            "--- After Processing ---\n",
            "'m long time musician long time user ernie ball strings electric guitars unfortunately ca n't endorse strings even though sound great light gauge saves fingers problem break humble opinion excessively easily put set broke g string within 2 days moderate use snapped near saddle n't think much replaced set within days noticed wrapping g string broken becoming unraveled near third fret bought 7 sets still 5 sets go honestly 'll probably go back elixirs gone since seem last longer update purchased strings 12/19 1/18 've broken 3 strings 3 different packs point really hate strings\n",
            "\n",
            "============================================================\n",
            "\n",
            "--- Before Processing ---\n",
            "Great quality patch cables!\n",
            "\n",
            "I wanted something other than those cheap blue, green, red and yellow patch cables because I didn't want to replace the cables on my pedal board several times a year.\n",
            "\n",
            "I purchased these because they had excellent reviews and I'd like to add to those because they are great cables, well built, feel sturdy and have a great quality feel. I get great tones from my pedals and amp and they were the perfect link for me. If you're on the fence about the Hosa CPE-118, go for it, you will be happy with them!\n",
            "--- After Processing ---\n",
            "great quality patch cables wanted something cheap blue green red yellow patch cables n't want replace cables pedal board several times year purchased excellent reviews 'd like add great cables well built feel sturdy great quality feel get great tones pedals amp perfect link 're fence hosa cpe-118 go happy\n",
            "\n",
            "============================================================\n",
            "\n",
            "--- Before Processing ---\n",
            "Looks great! Replaced the stock neck plate on my Squier standard telecaster.\n",
            "--- After Processing ---\n",
            "looks great replaced stock neck plate squier standard telecaster\n",
            "\n",
            "============================================================\n",
            "\n",
            "--- Before Processing ---\n",
            "These cables are perfect for pedalboards that need a little more reach than average. They aren't as cheap looking as other budget cables.\n",
            "\n",
            "Admittedly I'm not a cable snob, but I don't care for crap cables either. These are about the same quality as the Fender cables you see in the fishbowls of the big-brother music stores. One thing to keep in mind, if you mount your cables into a pedalboard and use zip ties, the cables don't move much and are less prone to breakage like your regular guitar cables. The DIY kits by George L and Planet Waves are ridiculously priced, and these are a great alternative.\n",
            "--- After Processing ---\n",
            "cables perfect pedalboards need little reach average n't cheap looking budget cables admittedly 'm cable snob n't care crap cables either quality fender cables see fishbowls big-brother music stores one thing keep mind mount cables pedalboard use zip ties cables n't move much less prone breakage like regular guitar cables diy kits george l planet waves ridiculously priced great alternative\n",
            "\n",
            "============================================================\n",
            "\n",
            "--- Before Processing ---\n",
            "A solid strap that I used on my Strat.  Very nice.  It's not flashy, which fits my style well.  Works well, solidly made.  No worries that the guitar is going to come off while playing.  I've taken some shots of the Poly Locks, check out the mechanism.  It's pretty nice.  Definitely will do what you want and last a good long while.\n",
            "--- After Processing ---\n",
            "solid strap used strat nice 's flashy fits style well works well solidly made worries guitar going come playing 've taken shots poly locks check mechanism 's pretty nice definitely want last good long\n",
            "\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***ANSWER 2***"
      ],
      "metadata": {
        "id": "U965JNDWluR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "class QueryProcessor:\n",
        "    def __init__(self, index_obj):\n",
        "        self.index = index_obj\n",
        "\n",
        "    def process_query(self, terms, operators):\n",
        "        if not terms or not operators or not self.index:\n",
        "            return set()\n",
        "\n",
        "        results = set(self.index.get(terms[0], []))\n",
        "        for i, operator in enumerate(operators):\n",
        "            if i + 1 < len(terms):\n",
        "                next_terms = terms[i + 1]\n",
        "                postings = self.index.get(next_terms, [])\n",
        "                if operator == 'AND':\n",
        "                    results &= set(postings)\n",
        "                elif operator == 'OR':\n",
        "                    results |= set(postings)\n",
        "                elif operator == 'AND NOT':\n",
        "                    results -= set(postings)\n",
        "                elif operator == 'OR NOT':\n",
        "                    results = results.union(set(self.index.keys()) - postings)\n",
        "        return results\n",
        "\n",
        "    def execute_queries(self, num_queries, queries):\n",
        "        print(\"\\n\" + \"=\"*100 + \"\\n\")  # Add separator\n",
        "\n",
        "        print(\"b. Output:\")\n",
        "        for i in range(num_queries):\n",
        "            query_input = queries[i][0]\n",
        "            operator_input = queries[i][1]\n",
        "            terms = preprocess_query(query_input)\n",
        "            operators = operator_input.split(',')\n",
        "\n",
        "            if len(terms) != len(operators) + 1:\n",
        "                print(f\"Error: Number of operations should be one less than the number of terms in the query inputted.\")\n",
        "                continue\n",
        "\n",
        "            query_display = ' '.join([f\"{term} {op}\" for term, op in zip(terms[:-1], operators)])\n",
        "            query_display += f\" {terms[-1]}\"  # Add the last term separately\n",
        "\n",
        "            results = self.process_query(terms, operators)\n",
        "\n",
        "            print(f\"Query{i+1}: {query_display}\")\n",
        "            print(f\"Number of documents retrieved for query {i+1}: {len(results)}\")\n",
        "            print(f\"Names of the documents retrieved for query {i+1}: {', '.join(results) if results else 'None'}\")\n",
        "\n",
        "\n",
        "\n",
        "class InvertedIndex:\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.idx = self.build_idx()\n",
        "\n",
        "    def build_idx(self):\n",
        "        idx = {}\n",
        "        for file in os.listdir(self.data_dir):\n",
        "            if file.endswith(\".txt\"):\n",
        "                path = os.path.join(self.data_dir, file)\n",
        "                with open(path, 'r', encoding='utf-8') as content:\n",
        "                    for term in content.read().split():\n",
        "                        if term not in idx:\n",
        "                            idx[term] = [file]\n",
        "                        elif file not in idx[term]:\n",
        "                            idx[term].append(file)\n",
        "        return idx\n",
        "\n",
        "    def save_index(self, filepath):\n",
        "        with open(filepath, 'wb') as file:\n",
        "            pickle.dump(self.idx, file)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_index(filepath):\n",
        "        with open(filepath, 'rb') as file:\n",
        "            return pickle.load(file)\n",
        "\n",
        "\n",
        "def preprocess_query(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def main():\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "    src_dir = '/content/drive/My Drive/IR/text_files'\n",
        "    dest_dir = '/content/drive/MyDrive/IR/processed_files'\n",
        "\n",
        "    index = InvertedIndex(dest_dir)\n",
        "    index_file = 'index_file.pkl'\n",
        "    index.save_index(index_file)\n",
        "\n",
        "    loaded_index = InvertedIndex.load_index(index_file)\n",
        "\n",
        "    processor = QueryProcessor(loaded_index)\n",
        "\n",
        "    print(\"a. Input:\")\n",
        "    num_queries = int(input())\n",
        "    queries = []\n",
        "    for i in range(num_queries):\n",
        "        query_input = input( )\n",
        "        operator_input = input()\n",
        "        queries.append((query_input, operator_input))\n",
        "\n",
        "\n",
        "    processor.execute_queries(num_queries, queries)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "7oSd6qJplsfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0894ba89-9349-4bba-ce7c-e6d1a8739204"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a. Input:\n",
            "2\n",
            "Car bag in a canister\n",
            "OR, AND NOT\n",
            "Coffee brewing techniques in cookbook\n",
            "AND, OR NOT, OR\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "b. Output:\n",
            "Query1: car OR bag  AND NOT canister\n",
            "Number of documents retrieved for query 1: 31\n",
            "Names of the documents retrieved for query 1: file930_cleaned.txt, file404_cleaned.txt, file118_cleaned.txt, file459_cleaned.txt, file860_cleaned.txt, file864_cleaned.txt, file313_cleaned.txt, file738_cleaned.txt, file73_cleaned.txt, file797_cleaned.txt, file166_cleaned.txt, file746_cleaned.txt, file466_cleaned.txt, file264_cleaned.txt, file942_cleaned.txt, file699_cleaned.txt, file981_cleaned.txt, file886_cleaned.txt, file698_cleaned.txt, file686_cleaned.txt, file174_cleaned.txt, file3_cleaned.txt, file542_cleaned.txt, file863_cleaned.txt, file573_cleaned.txt, file682_cleaned.txt, file956_cleaned.txt, file363_cleaned.txt, file780_cleaned.txt, file665_cleaned.txt, file892_cleaned.txt\n",
            "Query2: coffee AND brewing  OR NOT techniques  OR cookbook\n",
            "Number of documents retrieved for query 2: 0\n",
            "Names of the documents retrieved for query 2: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***ANSWER 3**"
      ],
      "metadata": {
        "id": "OMzpitrTXIKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "class QueryPositionalIndex:\n",
        "    def __init__(self, directory_path):\n",
        "        self.dir_path = directory_path\n",
        "        self.idx = self.build_index()\n",
        "\n",
        "    def build_index(self):\n",
        "        idx = {}\n",
        "        for filename in os.listdir(self.dir_path):\n",
        "            if filename.endswith(\".txt\"):\n",
        "                file_path = os.path.join(self.dir_path, filename)\n",
        "                with open(file_path, 'r', encoding='utf-8') as content:\n",
        "                    for position, term in enumerate(content.read().split()):\n",
        "                        postings = idx.setdefault(term, {})\n",
        "                        positions = postings.setdefault(filename, [])\n",
        "                        positions.append(position)\n",
        "        return idx\n",
        "\n",
        "    def save(self, file_path):\n",
        "        with open(file_path, 'wb') as file:\n",
        "            pickle.dump(self.idx, file)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(file_path):\n",
        "        with open(file_path, 'rb') as file:\n",
        "            return pickle.load(file)\n",
        "\n",
        "class PhraseQueryProcessor:\n",
        "    def __init__(self, index):\n",
        "        self.index = index\n",
        "\n",
        "    def find_docs(self, phrase):\n",
        "        terms = preprocess_text(phrase)\n",
        "        if not terms:\n",
        "            return set()\n",
        "\n",
        "        postings_lists = [self.index.get(term, {}) for term in terms]\n",
        "        if not all(postings_lists):\n",
        "            return set()\n",
        "\n",
        "        common_docs = set(postings_lists[0].keys())\n",
        "        for postings in postings_lists:\n",
        "            common_docs &= postings.keys()\n",
        "\n",
        "        return common_docs\n",
        "\n",
        "# Redundant function for preprocessing text\n",
        "def preprocess_text(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# Initialize and save positional index\n",
        "processed_files_folder = '/content/drive/My Drive/IR/processed_files'\n",
        "pos_index = QueryPositionalIndex(processed_files_folder)\n",
        "\n",
        "index_file = 'positional_index.pkl'\n",
        "pos_index.save(index_file)\n",
        "loaded_index = QueryPositionalIndex.load(index_file)\n",
        "\n",
        "processor = PhraseQueryProcessor(loaded_index)\n",
        "\n",
        "# Read number of queries\n",
        "print(\"a. Input\")\n",
        "num_queries = int(input(\"Enter the number of queries: \"))\n",
        "queries = []\n",
        "for i in range(num_queries):\n",
        "    query_input = input(f\"Enter phrase query {i+1}: \")\n",
        "    queries.append(query_input)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Process queries\n",
        "print(\"b. Output\")\n",
        "for i, query_input in enumerate(queries):\n",
        "    result_docs = processor.find_docs(query_input)\n",
        "\n",
        "    print(f\"Number of documents retrieved for query {i+1} using positional index: {len(result_docs)}\")\n",
        "    print(f\"Names of documents retrieved for query {i+1} using positional index: {', '.join(result_docs) if result_docs else 'None'}\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq_9J6NHmWSm",
        "outputId": "25d30f7b-f6bd-4650-8a1d-42249c8d3897"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a. Input\n",
            "Enter the number of queries: 2\n",
            "Enter phrase query 1: Car bag in a canister\n",
            "Enter phrase query 2: Coffee brewing techniques in cookbook\n",
            "\n",
            "\n",
            "b. Output\n",
            "Number of documents retrieved for query 1 using positional index: 0\n",
            "Names of documents retrieved for query 1 using positional index: None\n",
            "\n",
            "\n",
            "Number of documents retrieved for query 2 using positional index: 0\n",
            "Names of documents retrieved for query 2 using positional index: None\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}